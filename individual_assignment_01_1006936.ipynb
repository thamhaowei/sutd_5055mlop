{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e6d310",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Collection and Prompt Engineering (10%)\n",
    "\n",
    "This assignment requires students to create a data set for training and evaluation of an SUTD chatbot for prospective students. The data set should contain documents about SUTD and question-answer pairs suitable for model training and evaluation.\n",
    "\n",
    "In addition to the data set, students should build a first prototype using only prompt engineering and foundation models available via APIs.\n",
    "\n",
    "Objectives:\n",
    "- Collect and curate documents related to SUTD (programs, admissions, campus, scholarships, student life, FAQs, policies).\n",
    "- Create a high-quality Q&A dataset suitable for training and evaluation.\n",
    "- Build a prompt-engineered chatbot prototype using a foundation model API with [Amazon Bedrock — What is Bedrock?](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html).\n",
    "- Evaluate prototype responses against the curated Q&A dataset.\n",
    "\n",
    "Deliverables:\n",
    "- Data artifact: documents (raw + cleaned), Q&A pairs (JSONL), metadata (sources, timestamps, licenses).\n",
    "- Notebook: end-to-end workflow (collection → cleaning → Q&A generation → prototype → evaluation).\n",
    "- Short report: methodology, data sources, prompt design, evaluation summary.\n",
    "\n",
    "Grading (10%):\n",
    "- Data quality and coverage (3%)\n",
    "- Q&A diversity, clarity, and correctness (3%)\n",
    "- Prototype design and prompt engineering (2%)\n",
    "- Evaluation thoroughness and analysis (2%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7347d",
   "metadata": {},
   "source": [
    "# Setup & Environment\n",
    "\n",
    "This notebook uses Python. If you use external APIs (e.g., AWS Bedrock), store credentials in environment variables and load them securely (do not hardcode keys).\n",
    "\n",
    "Recommended packages:\n",
    "- requests, beautifulsoup4, pandas, numpy, tqdm\n",
    "- scikit-learn (optional for baseline retrieval)\n",
    "- openai OR anthropic OR boto3 (choose one API path)\n",
    "- rouge-score OR evaluate (optional, for metrics)\n",
    "\n",
    "Tip: Use a virtual environment and a `.env` file or system keychain.\n",
    "\n",
    "Example environment variables:\n",
    "- OPENAI_API_KEY\n",
    "- ANTHROPIC_API_KEY\n",
    "- AWS_REGION + AWS credentials for Bedrock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1768ac",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Collect documents about SUTD from official, public sources (admissions pages, program descriptions, campus life, FAQ, scholarship info, policies). Respect robots.txt and terms of use; avoid overloading servers, and cache downloads locally.\n",
    "\n",
    "Suggested steps:\n",
    "1. Identify seed URLs and a scope definition (which pages to include).\n",
    "2. Fetch pages politely (rate limiting), parse text, and store raw HTML + cleaned text.\n",
    "3. Track metadata: URL, title, section, timestamp, retrieval status, license.\n",
    "4. Normalize and deduplicate content; segment long pages into sections.\n",
    "\n",
    "Artifacts:\n",
    "- data/raw/*.html\n",
    "- data/processed/*.md or *.txt\n",
    "- data/metadata.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c500d88",
   "metadata": {},
   "source": [
    "**1. Fetching raw HTML from SUTD FAQ website**\n",
    "\n",
    "To ground the chatbot in official university information, I collected content from the SUTD Undergraduate Admissions FAQ website.\n",
    "The FAQ content spans 9 paginated pages, each containing multiple question-and-answer entries.\n",
    "All source URLs were stored in data/seed_urls.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45aaae3",
   "metadata": {},
   "source": [
    "The pipeline is as follows:\n",
    "\n",
    "seed_urls.txt --> fetch_html.py --> raw HTML files + metadata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff93fc",
   "metadata": {},
   "source": [
    "**HTML to readable text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "163bfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f55a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admissions_undergraduate_faq_paged_1.html → admissions_undergraduate_faq_paged_1_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_2.html → admissions_undergraduate_faq_paged_2_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_3.html → admissions_undergraduate_faq_paged_3_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_4.html → admissions_undergraduate_faq_paged_4_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_5.html → admissions_undergraduate_faq_paged_5_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_6.html → admissions_undergraduate_faq_paged_6_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_7.html → admissions_undergraduate_faq_paged_7_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_8.html → admissions_undergraduate_faq_paged_8_faq.txt | extracted 10 Q&A\n",
      "admissions_undergraduate_faq_paged_9.html → admissions_undergraduate_faq_paged_9_faq.txt | extracted 3 Q&A\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEPARATOR = \"--------------\"\n",
    "\n",
    "def clean(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_faq_from_html(html: str):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    accordion = soup.select_one(\"section#accordion\")\n",
    "    if not accordion:\n",
    "        return []\n",
    "\n",
    "    qa_pairs = []\n",
    "\n",
    "    # Each FAQ item contains an h6 (question) and a div.richText (answer)\n",
    "    # We'll pair them by walking each question to its nearest following richText.\n",
    "    for h6 in accordion.select(\"h6\"):\n",
    "        q = h6.get_text(\" \", strip=True)\n",
    "        q = clean(q)\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        body = h6.find_parent()\n",
    "        # search forward in the DOM for the first answer block\n",
    "        ans_div = h6.find_next(\"div\", class_=\"richText\")\n",
    "        if not ans_div:\n",
    "            continue\n",
    "\n",
    "        # Keep paragraphs + links as text\n",
    "        a = ans_div.get_text(\"\\n\", strip=True)\n",
    "        a = clean(a)\n",
    "\n",
    "        # Guard against accidentally capturing empty/irrelevant blocks\n",
    "        if a:\n",
    "            qa_pairs.append((q, a))\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def write_qa_txt(qa_pairs, out_path: Path):\n",
    "    parts = []\n",
    "    for q, a in qa_pairs:\n",
    "        parts.append(SEPARATOR)\n",
    "        parts.append(q)\n",
    "        parts.append(\"\")        # empty line\n",
    "        parts.append(a)\n",
    "    out_text = \"\\n\".join(parts).rstrip()  # keep trailing separator optional\n",
    "    out_path.write_text(out_text, encoding=\"utf-8\")\n",
    "\n",
    "# Run over all downloaded HTML pages\n",
    "for html_path in sorted(RAW_DIR.glob(\"*.html\")):\n",
    "    html = html_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    qa = extract_faq_from_html(html)\n",
    "\n",
    "    out_path = OUT_DIR / f\"{html_path.stem}_faq.txt\"\n",
    "    write_qa_txt(qa, out_path)\n",
    "\n",
    "    print(f\"{html_path.name} → {out_path.name} | extracted {len(qa)} Q&A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "585a36fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 9 files → data/processed/sutd_undergrad_faq_all.txt\n"
     ]
    }
   ],
   "source": [
    "# Combine all 9 files into 1\n",
    "from pathlib import Path\n",
    "\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "combined_path = PROCESSED_DIR / \"sutd_undergrad_faq_all.txt\"\n",
    "\n",
    "faq_files = sorted(PROCESSED_DIR.glob(\"*_faq.txt\"))\n",
    "\n",
    "combined_text = \"\\n\\n\".join(f.read_text(encoding=\"utf-8\") for f in faq_files)\n",
    "combined_path.write_text(combined_text, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Combined {len(faq_files)} files → {combined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b51b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 83\n"
     ]
    }
   ],
   "source": [
    "# Count questions to verify\n",
    "combined_path = \"data/processed/sutd_undergrad_faq_all.txt\"\n",
    "\n",
    "with open(combined_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "num_questions = text.count(\"--------------\")\n",
    "\n",
    "print(\"Number of questions:\", num_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "058f05b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved admissions_undergraduate_faq_paged_5_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_4_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_6_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_7_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_2_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_3_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_8_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_1_faq.txt → archive/\n",
      "Moved admissions_undergraduate_faq_paged_9_faq.txt → archive/\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Move the old 9 pages (now combined) into archive folder\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "processed_dir = Path(\"data/processed\")\n",
    "archive_dir = Path(\"data/archive\")\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# move only the individual page files, not the combined one\n",
    "for file in processed_dir.glob(\"*_faq.txt\"):\n",
    "    if file.name != \"sutd_undergrad_faq_all.txt\":\n",
    "        dest = archive_dir / file.name\n",
    "        shutil.move(str(file), str(dest))\n",
    "        print(f\"Moved {file.name} → archive/\")\n",
    "        \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41576750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No-link Q&A: 44\n",
      "With-link Q&A: 39\n",
      "Links extracted: 91\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "\n",
    "faq_with_links = []\n",
    "faq_no_links = []\n",
    "link_records = []\n",
    "\n",
    "BASE = \"https://www.sutd.edu.sg\"\n",
    "\n",
    "for html_file in RAW_DIR.glob(\"*.html\"):\n",
    "    soup = BeautifulSoup(html_file.read_text(encoding=\"utf-8\", errors=\"ignore\"), \"html.parser\")\n",
    "\n",
    "    accordion = soup.select_one(\"section#accordion\")\n",
    "    if not accordion:\n",
    "        continue\n",
    "\n",
    "    for h6 in accordion.select(\"h6\"):\n",
    "        question = h6.get_text(\" \", strip=True)\n",
    "\n",
    "        ans_div = h6.find_next(\"div\", class_=\"richText\")\n",
    "        if not ans_div:\n",
    "            continue\n",
    "\n",
    "        # extract answer text\n",
    "        answer_text = ans_div.get_text(\"\\n\", strip=True)\n",
    "\n",
    "        # extract hyperlinks\n",
    "        links = []\n",
    "        for a in ans_div.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if href.startswith(\"/\"):\n",
    "                href = BASE + href\n",
    "            links.append(href)\n",
    "\n",
    "        entry = f\"--------------\\n{question}\\n\\n{answer_text}\\n\"\n",
    "\n",
    "        if links:\n",
    "            faq_with_links.append(entry)\n",
    "            for l in links:\n",
    "                link_records.append({\"question\": question, \"link\": l})\n",
    "        else:\n",
    "            faq_no_links.append(entry)\n",
    "\n",
    "# save outputs\n",
    "Path(\"data/processed/faq_no_links.txt\").write_text(\"\".join(faq_no_links), encoding=\"utf-8\")\n",
    "Path(\"data/archive/faq_with_links.txt\").write_text(\"\".join(faq_with_links), encoding=\"utf-8\")\n",
    "\n",
    "pd.DataFrame(link_records).to_csv(\"data/archive/faq_links_to_visit.csv\", index=False)\n",
    "\n",
    "print(\"No-link Q&A:\", len(faq_no_links))\n",
    "print(\"With-link Q&A:\", len(faq_with_links))\n",
    "print(\"Links extracted:\", len(link_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c4e18",
   "metadata": {},
   "source": [
    "# Q&A Generation\n",
    "\n",
    "Create question-answer pairs suitable for model training and evaluation. Aim for coverage: admissions eligibility, deadlines, programs, curriculum, scholarships, housing, student life, application process, contact channels.\n",
    "\n",
    "Options:\n",
    "- Manual authoring from authoritative sources (preferred for correctness).\n",
    "- LLM-assisted generation using your curated documents, followed by human validation.\n",
    "\n",
    "Guidelines:\n",
    "- Keep answers concise and factual; include references (URL, section) in metadata.\n",
    "- Avoid speculative or outdated info; include retrieval timestamp.\n",
    "- Provide diverse phrasings and difficulty levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here (if appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6d73",
   "metadata": {},
   "source": [
    "# Dataset Assembly\n",
    "\n",
    "Format the Q&A into a machine-learning friendly structure (JSONL recommended). Include:\n",
    "- id, question, answer\n",
    "- source (URL/file), retrieved_at timestamp\n",
    "- split (train/test/dev), topic/category\n",
    "\n",
    "Ensure a clear train/test split with no leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc35500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here (if appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a25e97",
   "metadata": {},
   "source": [
    "# Prompt-Engineered Prototype\n",
    "\n",
    "Build a simple chatbot that answers prospective student questions using:\n",
    "- A concise system prompt (tone: helpful, factual, official).\n",
    "- Lightweight retrieval from your curated documents for grounding.\n",
    "- A foundation model API (OpenAI, Anthropic, or AWS Bedrock).\n",
    "\n",
    "Note: Do not include private keys in the notebook. Use environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc21231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here (if appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0d28",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Evaluate prototype answers against the Q&A dataset. Use a simple metric (e.g., token overlap or string similarity) and manual spot checks.\n",
    "\n",
    "Suggested metrics:\n",
    "- Exact match / normalized overlap\n",
    "- ROUGE-L (optional)\n",
    "- Human review with rubric (clarity, correctness, completeness, source alignment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here (if appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba35fee",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "This concludes Individual assignment 1.\n",
    "\n",
    "Please submit this notebook with your answers and the generated output cells as a **Jupyter notebook file** via github.\n",
    "\n",
    "\n",
    "Every student should do the following submission steps:\n",
    "1. Create a private github repository **sutd_5055mlops** under your github user.\n",
    "2. Add your instructors as collaborator: ddahlmeier, bearwithchris and MarkHershey\n",
    "3. Save your submission as `individual_assignment_01_StudentID`.ipynb (replace StudentID with your student ID)\n",
    "4. Push the submission files to your repo \n",
    "5. Submit the link to the repo via eDimensions \n",
    "\n",
    "\n",
    "\n",
    "**Assignment due 27 Feb (Fri) 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea384d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
